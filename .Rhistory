hessian = T,
control = list(fnscale=-1))
# maximum a posterirori
MAP <- optimal$par
# the optim gives back also an "estimate" of the Hessian matrix around the maximum
# Through this estimate we define the matrix of variance and covariance of the proposal.
# Indeed, the second derivative of the negative log-likelihood evaluated at the maximum likelihood
# estimates (MLE) is the observed Fisher information and the inverse of the Fisher information matrix
# is an estimator of the covariance matrix
Sigma <- - solve(optimal$hessian)
log_proposal <- c()
for(i in 1:nrow(expanded_grid)){
log_proposal[i] <- dmvnorm(expanded_grid[i,], mean = MAP, sigma = Sigma, log = T)
}
plot_df_2 <- data.frame(mu = expanded_grid[,1], lam = expanded_grid[,2], lpost = log_proposal)
ggplot(plot_df, mapping = aes(x = mu, y = lam, z = lpost)) +
geom_contour() +
geom_contour(data = plot_df_2, col = 2, lty = 2) +
theme_bw() +
xlab("mu") +
ylab("lambda") +
ggtitle("log-posterior distribution")
cauchy_MH <- function(niter, burnin,
theta0, Sigma, mu1, mu2, sig1, sig2){
# niter, burnin: iterations
# theta0 initial state of the chain
# Sig: proposal's var-cov matrix
# mu1, mu2, sig2, sig2: hyperparameters
# define the matrix that contains the output MCMC sample
theta_output <- matrix(nrow = (niter-burnin), ncol = 2)
# number of accepted moves
nacp = 0
pb <- txtProgressBar(min = 1, max = niter, initial = 1, style = 3)
# Start from theta0
for(i in 1:(niter))
{
theta_temp <- as.vector(rmvnorm(1, mean = theta0, sigma = Sigma)) # propose theta_temp: the mean is given by the previous value
## Compute the logarithm of the probability of accepting the transition from th0 to theta_temp
# First consider the log of accept/reject ratio. Numerator:
lacp <- cauchy_log_post(th = theta_temp, y = data, mu1 = mu1, sig1 = sig1, mu2 = mu2, sig2 = sig2)
# Denominator:
lacp <- lacp - cauchy_log_post(th = theta0, y = data, mu1 = mu1, sig1 = sig1, mu2 = mu2, sig2 = sig2)
lacp <- min(0, lacp)
# Note: The proposal is symmetrical and therefore does not appear in the value of acceptance / rejection!
lgu <- log(runif(1))
## if u < acp accept the move
if(lgu < lacp)
{
theta0 <- theta_temp
nacp = nacp + 1
}
# otherwise remain in the same state
if(i > burnin )
{
theta_output[(i-burnin),] = theta0
}
setTxtProgressBar(pb, i)
}
close(pb)
cat("Acceptance rate =", nacp/niter, "\n")
return(theta_output)
}
plot_df_2 <- data.frame(mu = expanded_grid[,1], lam = expanded_grid[,2], lpost = log_proposal)
ggplot(plot_df, mapping = aes(x = mu, y = lam, z = lpost)) +
geom_contour() +
geom_contour(data = plot_df_2, col = 2, lty = 2) +
theme_bw() +
xlab("mu") +
ylab("lambda") +
ggtitle("log-posterior distribution")
cauchy_MH <- function(niter, burnin,
theta0, Sigma, mu1, mu2, sig1, sig2){
# niter, burnin: iterations
# theta0 initial state of the chain
# Sig: proposal's var-cov matrix
# mu1, mu2, sig2, sig2: hyperparameters
# define the matrix that contains the output MCMC sample
theta_output <- matrix(nrow = (niter-burnin), ncol = 2)
# number of accepted moves
nacp = 0
pb <- txtProgressBar(min = 1, max = niter, initial = 1, style = 3)
# Start from theta0
for(i in 1:(niter))
{
theta_temp <- as.vector(rmvnorm(1, mean = theta0, sigma = Sigma)) # propose theta_temp: the mean is given by the previous value
## Compute the logarithm of the probability of accepting the transition from th0 to theta_temp
# First consider the log of accept/reject ratio. Numerator:
lacp <- cauchy_log_post(th = theta_temp, y = data, mu1 = mu1, sig1 = sig1, mu2 = mu2, sig2 = sig2)
# Denominator:
lacp <- lacp - cauchy_log_post(th = theta0, y = data, mu1 = mu1, sig1 = sig1, mu2 = mu2, sig2 = sig2)
lacp <- min(0, lacp)
# Note: The proposal is symmetrical and therefore does not appear in the value of acceptance / rejection!
lgu <- log(runif(1))
## if u < acp accept the move
if(lgu < lacp)
{
theta0 <- theta_temp
nacp = nacp + 1
}
# otherwise remain in the same state
if(i > burnin )
{
theta_output[(i-burnin),] = theta0
}
setTxtProgressBar(pb, i)
}
close(pb)
cat("Acceptance rate =", nacp/niter, "\n")
return(theta_output)
}
#------------------------------------
# RUN
# fix the seed
set.seed(42)
# fix the parameters of the Metropolis
niter = 30000
burnin = 20000
# At the end the chain will contain
# (niter-burnin) = 5000 observations
theta0 = MAP
theta_post <- cauchy_MH(niter = niter, burnin = burnin, theta0 = theta0, Sigma = Sigma,
mu1 = mu1, mu2 = mu2, sig1 = sig1, sig2 = sig2)
# CODA --> package to do diagnostic in mcmc
# with mcmc command you import the output as a "coda object" that whose properties are exploited by the library
mcmc_theta_post <- mcmc(theta_post, start = burnin + 1, end = niter)
summary(mcmc_theta_post)
# AUTOCORRELATION PLOTS
plot(mcmc_theta_post)
acfplot(mcmc_theta_post)
cumuplot(mcmc_theta_post)
effectiveSize(mcmc_theta_post)
geweke.diag(mcmc_theta_post)
geweke.plot(mcmc_theta_post, frac1 = 0.1, frac2 = 0.5, nbins = 20)
mu_post <- theta_post[,1]
lam_post <- theta_post[,2]
S <- length(lam_post) # number of final iterations
grid_x = seq(-150, 250, length = 500) # grid on which I want to evaluate the predictive
Dens_pred_matrix <- matrix(0, ncol = 500, nrow = S) # Matrix that will contain the values
# Scale parameter:
sig_post = exp(lam_post)
# Column by column
for(i in 1:500){
z <- (grid_x[i] - mu_post)/sig_post
# Density in the logarithmic scale
Dens_pred_matrix[,i] <- -log(pi) - lam_post - log(1+z^2)
}
Dens_pred_matrix <- exp(Dens_pred_matrix)
# plot
p <- ggplot() +
geom_line(data = data.frame(x = grid_x, y = Dens_pred_matrix[1,]),
mapping = aes(x = x, y = y), alpha = 0.3) +
theme_bw() +
ylab("density")
# just the first 250
for(i in 2:250){
p <- p + geom_line(data = data.frame(x = grid_x, y = Dens_pred_matrix[i,]),
mapping = aes(x = x, y = y), alpha = 0.3)
}
# posterior mean of the density and quantiles
Dens_pred <- apply(Dens_pred_matrix, 2, mean)
Dens_pred_quant <- apply(Dens_pred_matrix, 2, quantile, prob = c(0.05,0.95))
# add the lasts on the plot
p <- p + geom_line(data = data.frame(x = grid_x, y = Dens_pred),
mapping = aes(x = x, y = y), col = 3, lwd = 1) +
geom_line(data = data.frame(x = grid_x, y = Dens_pred_quant[1,]),
mapping = aes(x = x, y = y), col = 3, lty = 2, lwd = 1) +
geom_line(data = data.frame(x = grid_x, y = Dens_pred_quant[2,]),
mapping = aes(x = x, y = y), col = 3, lty = 2, lwd = 1)
# plot
p <- ggplot() +
geom_line(data = data.frame(x = grid_x, y = Dens_pred_matrix[1,]),
mapping = aes(x = x, y = y), alpha = 0.3) +
theme_bw() +
ylab("density")
x11()
p <- ggplot() +
geom_line(data = data.frame(x = grid_x, y = Dens_pred_matrix[1,]),
mapping = aes(x = x, y = y), alpha = 0.3) +
theme_bw() +
ylab("density")
# just the first 250
for(i in 2:250){
p <- p + geom_line(data = data.frame(x = grid_x, y = Dens_pred_matrix[i,]),
mapping = aes(x = x, y = y), alpha = 0.3)
}
library(ggplot2)
library(ggpubr)
library(LearnBayes)
library(mvtnorm)
library(coda)
x11()
p <- ggplot() +
geom_line(data = data.frame(x = grid_x, y = Dens_pred_matrix[1,]),
mapping = aes(x = x, y = y), alpha = 0.3) +
theme_bw() +
ylab("density")
gamma_vec = c(0.01, 1, 100)
niter = 11000
burnin = 10000
for(l in 1:length(gamma_vec)){
gamma = gamma_vec[l]
cat("--------- gamma = ", gamma, "-------------\n")
theta_post_temp <- cauchy_MH(niter = niter, burnin = burnin, theta0 = theta0, Sigma = gamma * Sigma,
mu1 = mu1, mu2 = mu2, sig1 = sig1, sig2 = sig2)
p1 <- ggplot() +
geom_contour(data = plot_df, mapping = aes(x = mu, y = lam, z = lpost)) +
geom_line(data = as.data.frame(theta_post_temp), mapping = aes(x = V1, y = V2)) +
theme_bw() +
xlab("mu") +
ylab("lambda") +
ggtitle("log-posterior distribution")
p2 <- ggplot()+
geom_line(data = data.frame(x = 1:(niter - burnin), y = theta_post_temp[,1]),
mapping = aes(x = x, y = y)) +
theme_bw() +
xlab("iter") +
ylab("mu")
p3 <- ggplot()+
geom_line(data = data.frame(x = 1:(niter - burnin), y = theta_post_temp[,1]),
mapping = aes(x = x, y = y)) +
theme_bw() +
xlab("iter") +
ylab("lambda")
print(ggarrange(p1, ggarrange(p2, p3, nrow = 2), ncol = 2))
Sys.sleep(10)
}
fx <- function(x){
# mixture of two normal distributions
return(0.2 * dnorm(x, -2.5, 1) + 0.8 * dnorm(x, 2.5, 1))
}
curve(fx, xlim = c(-6, 6))
bimodalN_MH <- function(niter, burnin,
x0, s2){
# initialize the algo
x_output <- c()
nacp = 0
pb <- txtProgressBar(min = 1, max = niter, initial = 1, style = 3)
for(i in 1:(niter))
{
# propose a new value
x_temp <- rnorm(1, mean = x0, sd = sqrt(s2))
# compute the acceptance ratio
acc_ratio <- min(1, fx(x_temp) / fx(x0))
if(runif(1) <= acc_ratio){
# if accepted update x0 and nacp
x0 <- x_temp
nacp = nacp + 1
}
# if the burnin phase is ended
if(i > burnin){
# return the value
x_output <- c(x_output, x0)
}
setTxtProgressBar(pb, i)
}
close(pb)
cat("Acceptance rate =", nacp/niter, "\n")
return(x_output)
}
set.seed(42)
curve_df <- data.frame(x = seq(-6, 6, length.out = 1000), y = fx(seq(-6, 6, length.out = 1000)))
plot_df <- data.frame(x = bimodalN_MH(15000, 10000, 0, 0.1)) # variance for the proposal: 0.1 --> super concentrated
# with this one I'll completely ignore a tail --> this because I am super stuck in part of the support
plot_df2 <- data.frame(x = bimodalN_MH(15000, 10000, 0, 6)) # variance for the proposal: 6
fx <- function(x){
# mixture of two normal distributions
return(0.2 * dnorm(x, -2.5, 1) + 0.8 * dnorm(x, 2.5, 1))
}
curve(fx, xlim = c(-6, 6))
bimodalN_MH <- function(niter, burnin,
x0, s2){
# initialize the algo
x_output <- c()
nacp = 0
pb <- txtProgressBar(min = 1, max = niter, initial = 1, style = 3)
for(i in 1:(niter))
{
# propose a new value
x_temp <- rnorm(1, mean = x0, sd = sqrt(s2))
# compute the acceptance ratio
acc_ratio <- min(1, fx(x_temp) / fx(x0))
if(runif(1) <= acc_ratio){
# if accepted update x0 and nacp
x0 <- x_temp
nacp = nacp + 1
}
# if the burnin phase is ended
if(i > burnin){
# return the value
x_output <- c(x_output, x0)
}
setTxtProgressBar(pb, i)
}
close(pb)
cat("Acceptance rate =", nacp/niter, "\n")
return(x_output)
}
set.seed(42)
curve_df <- data.frame(x = seq(-6, 6, length.out = 1000), y = fx(seq(-6, 6, length.out = 1000)))
plot_df <- data.frame(x = bimodalN_MH(15000, 10000, 0, 0.1)) # variance for the proposal: 0.1 --> super concentrated
# with this one I'll completely ignore a tail --> this because I am super stuck in part of the support
plot_df2 <- data.frame(x = bimodalN_MH(15000, 10000, 0, 6)) # variance for the proposal: 6
p1 <- ggplot() +
geom_histogram(data = plot_df, mapping = aes(x, stat(density)), alpha = 0.3, col = 1, bins = 60) +
geom_line(data = curve_df, mapping = aes(x = x, y = y), col = 2)+
theme_bw()
p2 <- ggplot()+
geom_line(data = plot_df, mapping = aes(x = 1:5000, y = x)) +
theme_bw() +
xlab("iter") +
ylab("x")
ggarrange(p1, p2, ncol = 2)
p3 <- ggplot() +
geom_histogram(data = plot_df2, mapping = aes(x, stat(density)), alpha = 0.3, col = 1, bins = 60) +
geom_line(data = curve_df, mapping = aes(x = x, y = y), col = 2)+
theme_bw()
p4 <- ggplot()+
geom_line(data = plot_df2, mapping = aes(x = 1:5000, y = x)) +
theme_bw() +
xlab("iter") +
ylab("x")
ggarrange(p3, p4, ncol = 2)
coda_obj <- mcmc(cbind(plot_df$x, plot_df2$x))
acfplot(coda_obj)
effectiveSize(coda_obj)
# proportional to a multivariate t-student
f_multi <- function(x, m, S, gdl){
(1 + t(x - m) %*% solve(S) %*% (x - m) / gdl)^(- (gdl + length(x))/2)
}
adapt_MH <- function(niter, burnin, m, S, gdl, x0, Sigma = NULL, beta = 0.05, adapt = T){
# initialize the algo
d <- length(x0) # set up dimension
x_res_temp <- matrix(x0, ncol = d)
nacp = 0
pb <- txtProgressBar(min = 1, max = niter, initial = 1, style = 3)
for(i in 1:(niter))
{
# propose a new value
if(adapt == F){
x_temp <- as.vector(rmvnorm(1, mean = x0, sigma = Sigma))
} else {
if(i <= 2*d){ # for the first iterations, we just use the first component, because we don't have enough values to estimate the variance
Sigma_temp <- (0.1)^2 * diag(1, d) / d
} else {
Sigma_temp <- (1 - beta)^2 * (2.38)^2 / d * var(x_res_temp) + beta^2 * (0.1)^2 * rep(1, d) / d
}
x_temp <- as.vector(rmvnorm(1, mean = x0, sigma = Sigma_temp))
}
# compute the acceptance ratio
acc_ratio <- min(1, f_multi(x_temp, m, S, gdl) / f_multi(x0, m, S, gdl))
if(runif(1) <= acc_ratio){
# if accepted update x0 and nacp
x0 <- x_temp
nacp = nacp + 1
}
x_res_temp <- rbind(x_res_temp, x0)
setTxtProgressBar(pb, i)
}
close(pb)
cat("Acceptance rate =", nacp/niter, "\n")
return(x_res_temp[-c(1:(burnin+1)), ])
}
niter = 15000
burnin = 10000
sample_no_adapt <- adapt_MH(niter, burnin, m = rep(0, 10), S = diag(0.25, 10),
gdl = 15, x0 = rep(0, 10), Sigma = diag(0.5, 10), adapt = F)
sample_adapt <- adapt_MH(niter, burnin, m = rep(0, 10), S = diag(0.254, 10),
gdl = 15, x0 = rep(0, 10), adapt = T)
# not adaptive
plot_df_1 <- data.frame(x = sample_no_adapt[,1], y = sample_no_adapt[,2], iter = 1:(niter - burnin))
p1_no <- ggplot() +
geom_point(data = plot_df_1, mapping = aes(x = x, y = y)) +
theme_bw() +
xlab("x") +
ylab("y") +
ggtitle("joint distribution of the first two dimensions")
p2_no <- ggplot()+
geom_line(data = plot_df_1, mapping = aes(x = iter, y = x)) +
theme_bw() +
xlab("iter") +
ylab("x")
p3_no <- ggplot()+
geom_line(data = plot_df_1, mapping = aes(x = iter, y = y)) +
theme_bw() +
xlab("iter") +
ylab("y")
ggarrange(p1_no, ggarrange(p2_no, p3_no, nrow = 2), ncol = 2)
# adaptive
plot_df_2 <- data.frame(x = sample_adapt[,1], y = sample_adapt[,2], iter = 1:(niter - burnin))
p1_y <- ggplot() +
geom_point(data = plot_df_2, mapping = aes(x = x, y = y)) +
theme_bw() +
xlab("x") +
ylab("y") +
ggtitle("joint distribution of the first two dimensions")
p2_y <- ggplot()+
geom_line(data = plot_df_2, mapping = aes(x = iter, y = x)) +
theme_bw() +
xlab("iter") +
ylab("x")
p3_y <- ggplot()+
geom_line(data = plot_df_2, mapping = aes(x = iter, y = y)) +
theme_bw() +
xlab("iter") +
ylab("y")
ggarrange(p1_y, ggarrange(p2_y, p3_y, nrow = 2), ncol = 2)
library(LearnBayes)
library(mvtnorm)
library(coda)
library(ggplot2)
library(ggpubr)
biv_normal_gibbs <- function(niter, burnin, thin, x0, mu, sig, rho)
{
results <- matrix(0, nrow = round((niter - burnin) / thin), ncol = 2)
x <- x0
pb <- txtProgressBar(min = 1, max = niter, initial = 1, style = 3)
for(j in 1:niter)
{
x[1] <- rnorm(1, mean = mu[1] + sig[1] / sig[2] * rho * (x[2] - mu[2]),
sd = sqrt(1 - rho^2) * sig[1])
x[2] <- rnorm(1, mean = mu[2] + sig[2] / sig[1] * rho * (x[1] - mu[1]),
sd = sqrt(1 - rho^2) * sig[2])
if(j > burnin & (j - burnin) %% thin==0)
{
results[(j - burnin) / thin, ] <- x
}
setTxtProgressBar(pb, j)
}
close(pb)
return(results)
}
## set the parameters
mu <- c(2, 10)
sig <- c(3, 1)
rho <- 0.25
## var-cov matrix
Sig <- matrix(c(sig[1]^2, rho * sig[1] * sig[2],
rho * sig[1] * sig[2], sig[2]^2), byrow = T, nrow = 2)
Sig
# Visualize the target distribution
# Grids
gr_x1 <- seq(mu[1] - 3 * sig[1], mu[1] + 3 * sig[1], length = 100)
gr_x2 <- seq(mu[2] - 3 * sig[2], mu[2] + 3 * sig[2], length = 100)
ex_grid <- expand.grid(gr_x1, gr_x2)
dens <- apply(ex_grid, 1, function(x) dmvnorm(x, mean = mu, sigma = Sig))
plot_df <- data.frame(x = ex_grid[,1], y = ex_grid[,2], dens = dens)
ggplot(plot_df) +
geom_contour(mapping = aes(x = x, y = y, z = dens)) +
theme_bw()
### -------------------------------------------------------------------------
### RUN THE GIBBS SAMPLER
# set the parameters
niter <- 200000
burnin <- 100000
thin <- 10
x0 <- c(1, 1)
# simulation
set.seed(42)
X <- biv_normal_gibbs(niter = niter, burnin = burnin, thin = thin,
x0 = x0, mu = mu, sig = sig, rho = rho)
### -------------------------------------------------------------------------
# plot the sampled values
ggplot(plot_df) +
geom_point(data = data.frame(x = X[,1], y = X[,2]), mapping = aes(x = x, y = y)) +
geom_contour(mapping = aes(x = x, y = y, z = dens)) +
theme_bw()
### -------------------------------------------------------------------------
### DIAGNOSTIC IN CODA
X_mc <- mcmc(data = X, start = burnin + 1, end = niter, thin = thin)
plot(X_mc)
summary(X_mc)
# Autocovariances
acfplot(X_mc, lag.max = 30)
# (0.025; 0.5, 0.975) quantiles
cumuplot(X_mc)
# Effective sample size:
effectiveSize(X_mc)
dim(X_mc)[1]
set.seed(1)
x <- c(rpois(25, lambda = 5), rpois(40, lambda = 12))
sample_log_probs <- function(vals, probs){
t_probs <- sapply(1:length(probs), function(x) 1 / sum(exp(probs - probs[x])) )
sample(vals, size = 1, prob = t_probs)
}
library(fda)
library(fda.usc)
library(knitr)
library(fields)
library(compositions)
setwd("C:/Users/Teresa Bortolotti/Documents/R/bayes_project/Functional-BNP-clustering")
my_data = f.data$ausxSL
x11()
plot(my_data$argvals, my_data$data[1,], type = "l", ylim= c(-250,250), xlab=expression(paste("t","  (",mu,"s)")), ylab= "y(t)", lwd = 2)
load("data_extraction.RData")
my_data = f.data$ausxSL
x11()
plot(my_data$argvals, my_data$data[1,], type = "l", ylim= c(-250,250), xlab=expression(paste("t","  (",mu,"s)")), ylab= "y(t)", lwd = 2)
for(i in 2:26){
lines(my_data$argvals, my_data$data[i,], col=i)
}
curves <- my_data$data
t_ax <- as.matrix(1:1600)
# Plot of original functions
x11()
matplot(t_ax,t(curves), type='l', xlab='x', ylab='orig.func')
n <- dim(curves)[1]
m <- 4
pen_smooth0 <- matrix(0, dim(curves)[1], dim(curves)[2])
pen_smooth1 <- matrix(0, dim(curves)[1], dim(curves)[2])
basis <- create.bspline.basis(breaks = t_ax, norder=m)
functionalPar <- fdPar(fdobj=basis, Lfdobj=1, lambda=1e3)
load("smooth_1600b_1D_1e3.RData")
