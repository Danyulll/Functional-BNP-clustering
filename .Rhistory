plot_df <- data.frame(x = bimodalN_MH(15000, 10000, 0, 0.1)) # variance for the proposal: 0.1 --> super concentrated
# with this one I'll completely ignore a tail --> this because I am super stuck in part of the support
plot_df2 <- data.frame(x = bimodalN_MH(15000, 10000, 0, 6)) # variance for the proposal: 6
fx <- function(x){
# mixture of two normal distributions
return(0.2 * dnorm(x, -2.5, 1) + 0.8 * dnorm(x, 2.5, 1))
}
curve(fx, xlim = c(-6, 6))
bimodalN_MH <- function(niter, burnin,
x0, s2){
# initialize the algo
x_output <- c()
nacp = 0
pb <- txtProgressBar(min = 1, max = niter, initial = 1, style = 3)
for(i in 1:(niter))
{
# propose a new value
x_temp <- rnorm(1, mean = x0, sd = sqrt(s2))
# compute the acceptance ratio
acc_ratio <- min(1, fx(x_temp) / fx(x0))
if(runif(1) <= acc_ratio){
# if accepted update x0 and nacp
x0 <- x_temp
nacp = nacp + 1
}
# if the burnin phase is ended
if(i > burnin){
# return the value
x_output <- c(x_output, x0)
}
setTxtProgressBar(pb, i)
}
close(pb)
cat("Acceptance rate =", nacp/niter, "\n")
return(x_output)
}
set.seed(42)
curve_df <- data.frame(x = seq(-6, 6, length.out = 1000), y = fx(seq(-6, 6, length.out = 1000)))
plot_df <- data.frame(x = bimodalN_MH(15000, 10000, 0, 0.1)) # variance for the proposal: 0.1 --> super concentrated
# with this one I'll completely ignore a tail --> this because I am super stuck in part of the support
plot_df2 <- data.frame(x = bimodalN_MH(15000, 10000, 0, 6)) # variance for the proposal: 6
p1 <- ggplot() +
geom_histogram(data = plot_df, mapping = aes(x, stat(density)), alpha = 0.3, col = 1, bins = 60) +
geom_line(data = curve_df, mapping = aes(x = x, y = y), col = 2)+
theme_bw()
p2 <- ggplot()+
geom_line(data = plot_df, mapping = aes(x = 1:5000, y = x)) +
theme_bw() +
xlab("iter") +
ylab("x")
ggarrange(p1, p2, ncol = 2)
p3 <- ggplot() +
geom_histogram(data = plot_df2, mapping = aes(x, stat(density)), alpha = 0.3, col = 1, bins = 60) +
geom_line(data = curve_df, mapping = aes(x = x, y = y), col = 2)+
theme_bw()
p4 <- ggplot()+
geom_line(data = plot_df2, mapping = aes(x = 1:5000, y = x)) +
theme_bw() +
xlab("iter") +
ylab("x")
ggarrange(p3, p4, ncol = 2)
coda_obj <- mcmc(cbind(plot_df$x, plot_df2$x))
acfplot(coda_obj)
effectiveSize(coda_obj)
# proportional to a multivariate t-student
f_multi <- function(x, m, S, gdl){
(1 + t(x - m) %*% solve(S) %*% (x - m) / gdl)^(- (gdl + length(x))/2)
}
adapt_MH <- function(niter, burnin, m, S, gdl, x0, Sigma = NULL, beta = 0.05, adapt = T){
# initialize the algo
d <- length(x0) # set up dimension
x_res_temp <- matrix(x0, ncol = d)
nacp = 0
pb <- txtProgressBar(min = 1, max = niter, initial = 1, style = 3)
for(i in 1:(niter))
{
# propose a new value
if(adapt == F){
x_temp <- as.vector(rmvnorm(1, mean = x0, sigma = Sigma))
} else {
if(i <= 2*d){ # for the first iterations, we just use the first component, because we don't have enough values to estimate the variance
Sigma_temp <- (0.1)^2 * diag(1, d) / d
} else {
Sigma_temp <- (1 - beta)^2 * (2.38)^2 / d * var(x_res_temp) + beta^2 * (0.1)^2 * rep(1, d) / d
}
x_temp <- as.vector(rmvnorm(1, mean = x0, sigma = Sigma_temp))
}
# compute the acceptance ratio
acc_ratio <- min(1, f_multi(x_temp, m, S, gdl) / f_multi(x0, m, S, gdl))
if(runif(1) <= acc_ratio){
# if accepted update x0 and nacp
x0 <- x_temp
nacp = nacp + 1
}
x_res_temp <- rbind(x_res_temp, x0)
setTxtProgressBar(pb, i)
}
close(pb)
cat("Acceptance rate =", nacp/niter, "\n")
return(x_res_temp[-c(1:(burnin+1)), ])
}
niter = 15000
burnin = 10000
sample_no_adapt <- adapt_MH(niter, burnin, m = rep(0, 10), S = diag(0.25, 10),
gdl = 15, x0 = rep(0, 10), Sigma = diag(0.5, 10), adapt = F)
sample_adapt <- adapt_MH(niter, burnin, m = rep(0, 10), S = diag(0.254, 10),
gdl = 15, x0 = rep(0, 10), adapt = T)
# not adaptive
plot_df_1 <- data.frame(x = sample_no_adapt[,1], y = sample_no_adapt[,2], iter = 1:(niter - burnin))
p1_no <- ggplot() +
geom_point(data = plot_df_1, mapping = aes(x = x, y = y)) +
theme_bw() +
xlab("x") +
ylab("y") +
ggtitle("joint distribution of the first two dimensions")
p2_no <- ggplot()+
geom_line(data = plot_df_1, mapping = aes(x = iter, y = x)) +
theme_bw() +
xlab("iter") +
ylab("x")
p3_no <- ggplot()+
geom_line(data = plot_df_1, mapping = aes(x = iter, y = y)) +
theme_bw() +
xlab("iter") +
ylab("y")
ggarrange(p1_no, ggarrange(p2_no, p3_no, nrow = 2), ncol = 2)
# adaptive
plot_df_2 <- data.frame(x = sample_adapt[,1], y = sample_adapt[,2], iter = 1:(niter - burnin))
p1_y <- ggplot() +
geom_point(data = plot_df_2, mapping = aes(x = x, y = y)) +
theme_bw() +
xlab("x") +
ylab("y") +
ggtitle("joint distribution of the first two dimensions")
p2_y <- ggplot()+
geom_line(data = plot_df_2, mapping = aes(x = iter, y = x)) +
theme_bw() +
xlab("iter") +
ylab("x")
p3_y <- ggplot()+
geom_line(data = plot_df_2, mapping = aes(x = iter, y = y)) +
theme_bw() +
xlab("iter") +
ylab("y")
ggarrange(p1_y, ggarrange(p2_y, p3_y, nrow = 2), ncol = 2)
library(LearnBayes)
library(mvtnorm)
library(coda)
library(ggplot2)
library(ggpubr)
biv_normal_gibbs <- function(niter, burnin, thin, x0, mu, sig, rho)
{
results <- matrix(0, nrow = round((niter - burnin) / thin), ncol = 2)
x <- x0
pb <- txtProgressBar(min = 1, max = niter, initial = 1, style = 3)
for(j in 1:niter)
{
x[1] <- rnorm(1, mean = mu[1] + sig[1] / sig[2] * rho * (x[2] - mu[2]),
sd = sqrt(1 - rho^2) * sig[1])
x[2] <- rnorm(1, mean = mu[2] + sig[2] / sig[1] * rho * (x[1] - mu[1]),
sd = sqrt(1 - rho^2) * sig[2])
if(j > burnin & (j - burnin) %% thin==0)
{
results[(j - burnin) / thin, ] <- x
}
setTxtProgressBar(pb, j)
}
close(pb)
return(results)
}
## set the parameters
mu <- c(2, 10)
sig <- c(3, 1)
rho <- 0.25
## var-cov matrix
Sig <- matrix(c(sig[1]^2, rho * sig[1] * sig[2],
rho * sig[1] * sig[2], sig[2]^2), byrow = T, nrow = 2)
Sig
# Visualize the target distribution
# Grids
gr_x1 <- seq(mu[1] - 3 * sig[1], mu[1] + 3 * sig[1], length = 100)
gr_x2 <- seq(mu[2] - 3 * sig[2], mu[2] + 3 * sig[2], length = 100)
ex_grid <- expand.grid(gr_x1, gr_x2)
dens <- apply(ex_grid, 1, function(x) dmvnorm(x, mean = mu, sigma = Sig))
plot_df <- data.frame(x = ex_grid[,1], y = ex_grid[,2], dens = dens)
ggplot(plot_df) +
geom_contour(mapping = aes(x = x, y = y, z = dens)) +
theme_bw()
### -------------------------------------------------------------------------
### RUN THE GIBBS SAMPLER
# set the parameters
niter <- 200000
burnin <- 100000
thin <- 10
x0 <- c(1, 1)
# simulation
set.seed(42)
X <- biv_normal_gibbs(niter = niter, burnin = burnin, thin = thin,
x0 = x0, mu = mu, sig = sig, rho = rho)
### -------------------------------------------------------------------------
# plot the sampled values
ggplot(plot_df) +
geom_point(data = data.frame(x = X[,1], y = X[,2]), mapping = aes(x = x, y = y)) +
geom_contour(mapping = aes(x = x, y = y, z = dens)) +
theme_bw()
### -------------------------------------------------------------------------
### DIAGNOSTIC IN CODA
X_mc <- mcmc(data = X, start = burnin + 1, end = niter, thin = thin)
plot(X_mc)
summary(X_mc)
# Autocovariances
acfplot(X_mc, lag.max = 30)
# (0.025; 0.5, 0.975) quantiles
cumuplot(X_mc)
# Effective sample size:
effectiveSize(X_mc)
dim(X_mc)[1]
set.seed(1)
x <- c(rpois(25, lambda = 5), rpois(40, lambda = 12))
sample_log_probs <- function(vals, probs){
t_probs <- sapply(1:length(probs), function(x) 1 / sum(exp(probs - probs[x])) )
sample(vals, size = 1, prob = t_probs)
}
load('nlr_data.rda')
setwd("C:/Users/Teresa Bortolotti/Desktop/ProgettoBayesiana/Functional Data Analysis/R code for fdakma")
library(fdakma)
data(kma.data)
x <- kma.data$x # abscissas
y0 <- kma.data$y0 # evaluations of original functions
y1 <- kma.data$y1 # evaluations of original function first derivatives
matplot(t(x),t(y0), type='l', xlab='x', ylab='orig.func')
title ('Original functions')
fdakma_example_noallign_1der <- kma(
x=x, y0=y0, y1=y1, n.clust = 3,
warping.method = 'NOalignment',
similarity.method = 'd1.pearson',   #similarity is computed as the cosine between the first derivatives (correlation)
center.method = 'k-means'
)
fdakma_example <- kma(
x=x, y0=y0, y1=y1, n.clust = 2,
warping.method = 'affine',
similarity.method = 'd1.pearson',  #similarity is computed as the cosine between the first derivatives (correlation)
center.method = 'k-means'
#seeds = c(1,21) # if you feel like, you can give a little help to the algorithm...
)
kma.show.results(fdakma_example)
fdakma_example <- kma(
x=x, y0=y0, y1=y1, n.clust = 1,
warping.method = 'affine',
similarity.method = 'd1.pearson',  #similarity is computed as the cosine between the first derivatives (correlation)
center.method = 'k-means'
#seeds = c(1,21) # if you feel like, you can give a little help to the algorithm...
)
kma.show.results(fdakma_example)
# Total shifts and dilations applied to the original
# abscissa to obtain the aligned abscissa
fdakma_example$shift
fdakma_example$dilation
quartz(width=10, height=7)
plot(x, type="n", xlim=c(min(x),max(x)), ylim=c(min(x),max(x)+2), xlab="abscissa", ylab="warping")
title("Allignment affinities")
for(i in 1:30)(
abline(a=fdakma_example$shift[i],b=fdakma_example$dilation[i], col=fdakma_example_noallign_0der$labels[i])
)
fdakma_example_noallign_0der <- kma(
x=x, y0=y0, n.clust = 3,
warping.method = 'NOalignment',
similarity.method = 'd0.pearson',   #similarity is computed as the cosine between the original curves (correlation)
center.method = 'k-means'
)
kma.show.results(fdakma_example_noallign_0der)
quartz(width=10, height=7)
plot(x, type="n", xlim=c(min(x),max(x)), ylim=c(min(x),max(x)+2), xlab="abscissa", ylab="warping")
title("Allignment affinities")
for(i in 1:30)(
abline(a=fdakma_example$shift[i],b=fdakma_example$dilation[i], col=fdakma_example_noallign_0der$labels[i])
)
setwd('C:/Users/Teresa Bortolotti/Documents/R/bayes_project/Functional-BNP-clustering')
load('smooth_60b_nopenalization.RData')
t_ax = 1:1600
t_ax <- 1:1600
y0 <- X_smoothed
fdakma_example <- kma(
x=t_ax, y0=y0, n.clust = 1,
warping.method = 'affine',
similarity.method = 'd0.pearson',  #similarity is computed as the cosine between the first derivatives (correlation)
center.method = 'k-means'
#seeds = c(1,21) # if you feel like, you can give a little help to the algorithm...
)
kma.show.results(fdakma_example)
save(fdakma_example, file = 'd0_pearson.RData')
sono_loro <- fdakma_example$y0.centers.final
matplot(t_ax, sono_loro, type='l', main="Aligned Data")
matplot(t_ax, t(sono_loro), type='l', main="Aligned Data")
sono_loro <- fdakma_example$y0.center.orig
matplot(t_ax, t(sono_loro), type='l', main="Aligned Data")
sono_loro <- fdakma_example$y0
matplot(t_ax, t(sono_loro), type='l', main="Aligned Data")
help('kma')
sono_loro <- fdakma_example$x.final
matplot(t_ax, t(sono_loro), type='l', main="Aligned Data")
matplot(t_ax, t(sono_loro), type='l', main="Aligned Data", ylim = c(0,1600))
sono_loro[1,]
sono_loro[,1]
X_smoothed[,1]
new_t <- fdakma_example$x.final
Data = matrix( sapply( means,
function( m )( dnorm( time_grid, mean = m, sd = 0.05 ) ) ),
ncol = P, nrow = N, byrow = TRUE )
N = 30
t0 = 0
t1 = 1
P = 1e3 + 1
time_grid = seq( t0, t1, length.out = P )
means = round( runif( N,
t0 + (t1 - t0) / 8,
t1 - (t1 - t0) / 8 ), 3 )
Data = matrix( sapply( means,
function( m )( dnorm( time_grid, mean = m, sd = 0.05 ) ) ),
ncol = P, nrow = N, byrow = TRUE )
fD = fData( time_grid, Data )
# Piecewise linear warpings
template_warping = function( m )( c( time_grid[ time_grid <= 0.5 ] * m / 0.5,
( time_grid[ time_grid > 0.5 ]
- 0.5 ) * (1 - m ) / 0.5 + m ) )
warpings = matrix( sapply( means, template_warping ),
ncol = P,
nrow = N, byrow = TRUE )
par(mfrow = c(1, 3))
plot( fD,
main = 'Unregistered functions', xlab = 'actual grid', ylab = 'values' )
plot( wfD,
main = 'Warping functions', xlab = 'registered grid',
ylab = 'actual grid' )
wfD = fData( time_grid, warpings )
dev.new()
oldpar <- par(mfrow = c(1, 1))
par(mfrow = c(1, 3))
plot( fD,
main = 'Unregistered functions', xlab = 'actual grid', ylab = 'values' )
plot( wfD,
main = 'Warping functions', xlab = 'registered grid',
ylab = 'actual grid' )
library(roahd)
fD_warped = warp( fD, wfD )
fD = fData( time_grid, Data )
# Piecewise linear warpings
template_warping = function( m )( c( time_grid[ time_grid <= 0.5 ] * m / 0.5,
( time_grid[ time_grid > 0.5 ]
- 0.5 ) * (1 - m ) / 0.5 + m ) )
warpings = matrix( sapply( means, template_warping ),
ncol = P,
nrow = N, byrow = TRUE )
wfD = fData( time_grid, warpings )
fD_warped = warp( fD, wfD )
dev.new()
oldpar <- par(mfrow = c(1, 1))
par(mfrow = c(1, 3))
plot( fD,
main = 'Unregistered functions', xlab = 'actual grid', ylab = 'values' )
plot( wfD,
main = 'Warping functions', xlab = 'registered grid',
ylab = 'actual grid' )
wfD = fData(t_ax, new_t )
fD <- fData(t_ax, y0 )
wfD <- fData(t_ax, new_t )
fD_warped <- warp( fD, wfD )
x11()
plot(t_ax, new_t, type='l', main="Warping Functions", ylim = c(0,1600))
x11()
plot(t_ax, t(new_t), type='l', main="Warping Functions", ylim = c(0,1600))
x11()
plot( wfD,
main = 'Warping functions', xlab = 'registered grid',
ylab = 'actual grid' )
plot( fD_warped,
main = 'Warped functions', xlab = 'registered grid',
ylab = 'values' , ylim = c(0,1600))
x11()
plot( wfD,
main = 'Warping functions', xlab = 'registered grid',
ylab = 'actual grid', ylim = c(0,1600) )
x11()
plot( wfD,
main = 'Warping functions', xlab = 'registered grid',
ylab = 'actual grid', xlim=c(0,1600),ylim = c(0,1600) )
range(new_t)
range(t_ax)
set.seed( 1618033 )
N = 30
t0 = 0
t1 = 1
P = 1e3 + 1
time_grid = seq( t0, t1, length.out = P )
means = round( runif( N,
t0 + (t1 - t0) / 8,
t1 - (t1 - t0) / 8 ), 3 )
Data = matrix( sapply( means,
function( m )( dnorm( time_grid, mean = m, sd = 0.05 ) ) ),
ncol = P, nrow = N, byrow = TRUE )
fD = fData( time_grid, Data )
# Piecewise linear warpings
template_warping = function( m )( c( time_grid[ time_grid <= 0.5 ] * m / 0.5,
( time_grid[ time_grid > 0.5 ]
- 0.5 ) * (1 - m ) / 0.5 + m ) )
warpings = matrix( sapply( means, template_warping ),
ncol = P,
nrow = N, byrow = TRUE )
range(time_grid)
range(warpings)
t_ax <- 1:1600
y0 <- X_smoothed
#save(fdakma_example, file = 'd0_pearson_affine.RData')
load('d0_pearson_affine.RData')
kma.show.results(fdakma_example)
d0.Pearson.shift <- kma(
x=t_ax, y0=y0, n.clust = 1,
warping.method = 'shift',
similarity.method = 'd0.pearson',  #similarity is computed as the cosine between the first derivatives (correlation)
center.method = 'k-means'
#seeds = c(1,21) # if you feel like, you can give a little help to the algorithm...
)
t_ax <- 1:1600
y0 <- X_smoothed
load('smooth_60b_nopenalization.RData')
t_ax <- 1:1600
y0 <- X_smoothed
fdakma_example <- kma(
x=t_ax, y0=y0, n.clust = 1,
warping.method = 'affine',
similarity.method = 'd0.pearson',  #similarity is computed as the cosine between the first derivatives (correlation)
center.method = 'k-means'
#seeds = c(1,21) # if you feel like, you can give a little help to the algorithm...
)
d0.Pearson.shift <- kma(
x=t_ax, y0=y0, n.clust = 1,
warping.method = 'shift',
similarity.method = 'd0.pearson',  #similarity is computed as the cosine between the first derivatives (correlation)
center.method = 'k-means'
#seeds = c(1,21) # if you feel like, you can give a little help to the algorithm...
)
save(d0.L2.shift, file = 'd0_L2_shift.RData')
save(d0.Pearson.shift, file = 'd0_pearson_shift.RData')
kma.show.results(d0.Pearson.shift)
d0.L2.shift <- kma(
x=t_ax, y0=y0, n.clust = 1,
warping.method = 'shift',
similarity.method = 'd0.L2',
center.method = 'k-means'
#seeds = c(1,21) # if you feel like, you can give a little help to the algorithm...
)
save(d0.L2.shift, file = 'd0_L2_shift.RData')
kma.show.results(d0.L2.shift)
d0.L2.centered.shift <- kma(
x=t_ax, y0=y0, n.clust = 1,
warping.method = 'shift',
similarity.method = 'd0.L2,centered',
center.method = 'k-means'
#seeds = c(1,21) # if you feel like, you can give a little help to the algorithm...
)
d0.L2.centered.shift <- kma(
x=t_ax, y0=y0, n.clust = 1,
warping.method = 'shift',
similarity.method = 'd0.L2.centered',
center.method = 'k-means'
#seeds = c(1,21) # if you feel like, you can give a little help to the algorithm...
)
save(d0.L2.centered.shift, file = 'd0_L2_centered_shift.RData')
kma.show.results(d0.L2.centered.shift)
d0.pearson.affine <- kma(
x=t_ax, y0=y0, n.clust = 1,
warping.method = 'affine',
similarity.method = 'd0.pearson',
center.method = 'k-means'
)
save(d0.pearson.affine, file = 'd0_pearson_affine.RData')
kma.show.results(d0.pearson.affine)
#save(d0.Pearson.shift, file = 'd0_pearson_shift.RData')
load('d0_pearson_shift.RData')
kma.show.results(d0.Pearson.shift)
#save(d0.L2.shift, file = 'd0_L2_shift.RData')
load('d0_L2_shift.RData')
kma.show.results(d0.L2.shift)
kma.show.results(d0.L2.shift)
#save(d0.L2.centered.shift, file = 'd0_L2_centered_shift.RData')
load('d0_L2_centered_shift.RData')
kma.show.results(d0.L2.centered.shift)
d1.pearson.affine <- kma(
x=t_ax, y0=y0, n.clust = 1,
warping.method = 'affine',
similarity.method = 'd1.pearson',
center.method = 'k-means'
)
y1 <- X_smoothed1
d1.pearson.affine <- kma(
x=t_ax, y0=y0,y1=y1, n.clust = 1,
warping.method = 'affine',
similarity.method = 'd1.pearson',
center.method = 'k-means'
)
save(d1.pearson.affine, file = 'd1_pearson_affine.RData')
kma.show.results(d1.pearson.affine)
d1.L2.shift <- kma(
x=t_ax, y0=y0,y1=y1, n.clust = 1,
warping.method = 'shift',
similarity.method = 'd1.L2',
center.method = 'k-means'
)
save(d1.L2.shift, file = 'd1_L2_shift.RData')
kma.show.results(d1.L2.shift)
graphics.off()
View(X_smoothed)
